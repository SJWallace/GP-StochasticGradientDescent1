{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Jupyter notebook sample",
   "id": "a47aa6cd633e8e0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "893254af43a003c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "Finding the optimum time to go to the gym. There's historical time series data about how busy the college gym has been on a 10 minute basis available. We'll find out the best days and times to go to the gym\n",
    "\n"
   ],
   "id": "a4a97e03801796d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "gym_data = pd.read_csv('crowdness_gym_data.csv')",
   "id": "712496c79a8c6bfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(gym_data.shape)",
   "id": "9a8054702c7685ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "earliest_date = gym_data['date'].min()\n",
    "latest_date = gym_data['date'].max()\n",
    "\n",
    "print(f\"Earliest date: {earliest_date}\")\n",
    "print(f\"Latest date: {latest_date}\")"
   ],
   "id": "31e97bfbb26fac20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# List all unique timezones in the 'date' column\n",
    "timezones = gym_data['date'].str.extract(r'([+-]\\d{2}:\\d{2})')[0].unique()\n",
    "print(\"Timezones:\", timezones)"
   ],
   "id": "93e99f36f9003cda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The timestamps indicate some daylight savings going on. Let's check that",
   "id": "10e331b503a0da9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract the timezone from the 'date' column\n",
    "gym_data['timezone'] = gym_data['date'].str.extract(r'([+-]\\d{2}:\\d{2})')[0]\n",
    "\n",
    "# Detect transitions (where timezone shifts occur)\n",
    "gym_data['shift'] = gym_data['timezone'] != gym_data['timezone'].shift()\n",
    "\n",
    "# Get the specific dates where the timezone changes\n",
    "timezone_shift_dates = gym_data.loc[gym_data['shift'], 'date']\n",
    "print(\"Timezone shift dates:\")\n",
    "print(timezone_shift_dates)"
   ],
   "id": "c28272ff354a566b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a datetime column from the 'date' column\n",
    "gym_data['datetime'] = pd.to_datetime(gym_data['date'], utc=True)"
   ],
   "id": "ec2b05e6bb83fad6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Timezone location\n",
    "Given shifts between daylight savings time and standard time. It's matching the Pacific Time (PT) timezone. The college could be in one of 5 states in the US.\n",
    "- Colorado\n",
    "- Washington\n",
    "- Nevada\n",
    "- Oregon (in certain areas)\n",
    "- Idaho (in pan handle)\n",
    "\n",
    "Idaho and Oregon don't universally observe DST, because of their geography. Most of Idaho is in Mountain Time (MT) and there is only one college in the PT section of Idaho, and only one college in the MT section of Oregon. So it is likely the college gym data comes from Colorado, Washington, Nevada or Oregon."
   ],
   "id": "86001bd9c13551e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data structure\n",
    "The dataframe contains 12 columns and about 62184 observations that start mid August 2015 and go to March 2017, but there's clearly some missing data from the start of January 2017 to March 2017 we'll need to address. Finding the start of semester and during semester times is also fairly trivial to perform if we can look up the college semester dates for those years. In this case though, we don't know which college this gym is located at, so we'll need to extract that from existing fields\n",
    "\n",
    "The real data that's valuable in this data set is the three columns\n",
    "`number_people`, `date`, `temperature`\n",
    "\n",
    "Many of the columns are helpful as splitting out data from the date column. We could generate day of week, weekend, holiday, month, hour trivially from the date column. See below\n",
    "```\n",
    "# Create new columns from the 'datetime' column\n",
    "import datetime as dt\n",
    "\n",
    "gym_data['timestamp'] = gym_data['datetime'].dt.hour * 3600 + gym_data['datetime'].dt.minute * 60 + gym_data['datetime'].dt.second\n",
    "gym_data['dayofweek'] = gym_data['datetime'].dt.dayofweek\n",
    "gym_data['is_weekend'] = gym_data['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "gym_data['month'] = gym_data['datetime'].dt.month\n",
    "gym_data['hour'] = gym_data['datetime'].dt.hour\n",
    "```\n",
    "\n",
    "\n",
    "- **`number_people`**: this is the number of people at the gym at each observation. This will be our target variable or label.\n",
    "- **`date`**: a string value with the specific date and time information.\n",
    "- **`timestamp`**: an integer (int), with the number of seconds since the start of the day (00:00).\n",
    "- **`dayofweek`**: an integer (int). 0 is equal to Monday and 6 is equal to Sunday.\n",
    "- **`is_weekend`**: a Boolean value defining if this observation happened during a weekend. 1 for yes, 0 for no.\n",
    "- **`is_holiday`**: a Boolean value defining if the observation happened during a holiday. 1 for yes, 0 for no.\n",
    "- **`temperature`**: a float, defining the temperature during the day of the observation in Fahrenheit.\n",
    "- **`is_start_of_semester`**: a Boolean defining if the observation happened in the first 2 weeks of a semester. 1 for yes, 0 for no.\n",
    "- **`is_during_semester`**: a Boolean defining if the observation happened during the active semester. 1 for yes, 0 for no.\n",
    "- **`month`**: an integer (int) defining the month of the year. 1 is equal to January, 12 is equal to December.\n",
    "- **`hour`**: an integer (int) for the hour of the day from 0 to 23.\n",
    "\n",
    "## Semesters\n",
    "First let's find the start, end and durations of the semesters. This will be important for our analysis, as this is a college gym, many students will return home out of semester time, so gym attendance will drop when they aren't around."
   ],
   "id": "8f567b55c6d934df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Resample to daily frequency to get only one observation per day\n",
    "daily_gym_data = gym_data.resample('D', on='datetime').first()\n",
    "\n",
    "# Detect start of semesters - the first entry that is true in a series of true.\n",
    "semester_start_dates = daily_gym_data[\n",
    "    (daily_gym_data['is_start_of_semester'] == True) &\n",
    "    (daily_gym_data['is_start_of_semester'].shift(1) != True)\n",
    "].index\n"
   ],
   "id": "81e2699a5118ae76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate semester durations\n",
    "# The end of the semester is when is_during_semester switches from true to false\n",
    "semester_durations = []\n",
    "\n",
    "# Create an object to store semester start and end dates\n",
    "semester_intervals = []\n",
    "\n",
    "for start_date in semester_start_dates:\n",
    "    # Ensure the 'is_during_semester' column is boolean\n",
    "    is_during_mask = (daily_gym_data.index >= start_date) & (daily_gym_data['is_during_semester'].astype(bool))\n",
    "\n",
    "    # Find the last day when is_during_semester is True after the start date\n",
    "    semester_end_date = daily_gym_data[is_during_mask & ~daily_gym_data['is_during_semester'].shift(-1).astype(bool)].index\n",
    "    if not semester_end_date.empty:\n",
    "        start = start_date.tz_convert('US/Pacific')  # Convert to Pacific Time\n",
    "        end = semester_end_date[0].tz_convert('US/Pacific')  # Convert to Pacific Time\n",
    "        duration = (end - start).days\n",
    "        semester_durations.append({\n",
    "            'year': start.year,\n",
    "            'start_date': start,\n",
    "            'end_date': end,\n",
    "            'duration_days': duration\n",
    "        })\n",
    "\n",
    "        # Append the start and end dates to the intervals object\n",
    "        semester_intervals.append((start, end))\n",
    "\n",
    "# Add a semester in 2015 that mirrors the 2016 January to May semester programmatically\n",
    "jan_may_2016_start = semester_durations[1]['start_date']  # 2016 January start\n",
    "duration_days = semester_durations[1]['duration_days']  # 2016 semester duration\n",
    "\n",
    "# Calculate the mirrored 2015 semester start and end dates\n",
    "jan_may_2015_start = jan_may_2016_start.replace(year=2015)\n",
    "jan_may_2015_end = jan_may_2015_start + pd.Timedelta(days=duration_days)\n",
    "\n",
    "# Convert the new semester's timestamps to Pacific Time\n",
    "jan_may_2015_start = jan_may_2015_start.tz_convert('US/Pacific')\n",
    "jan_may_2015_end = jan_may_2015_end.tz_convert('US/Pacific')\n",
    "\n",
    "# Add the new semester to the list\n",
    "semester_durations.insert(1, {\n",
    "    'year': 2015,\n",
    "    'start_date': jan_may_2015_start,\n",
    "    'end_date': jan_may_2015_end,\n",
    "    'duration_days': duration_days\n",
    "})\n",
    "semester_intervals.insert(1, (jan_may_2015_start, jan_may_2015_end))\n",
    "\n",
    "# Forecast the 2017 semester dates programmatically based on the 2016 semesters\n",
    "for semester in semester_durations:\n",
    "    if semester['year'] == 2016:\n",
    "        start_2017 = semester['start_date'].replace(year=2017)\n",
    "        end_2017 = semester['end_date'].replace(year=2017)\n",
    "\n",
    "        # Add the forecasted 2017 semesters to the list\n",
    "        semester_durations.append({\n",
    "            'year': 2017,\n",
    "            'start_date': start_2017,\n",
    "            'end_date': end_2017,\n",
    "            'duration_days': semester['duration_days']\n",
    "        })\n",
    "        semester_intervals.append((start_2017, end_2017))\n",
    "\n",
    "# semester_durations can be used for detailed information\n",
    "# semester_intervals can be used to add features to graphs\n",
    "semester_durations, semester_intervals"
   ],
   "id": "fb2b22b236e63668"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The data covers 3 semesters. The 4th semester that would start in 2017 is missing a lot of data so we'll have to ignore it. We've also estimated the dates of the first semester in 2015\n",
    "\n",
    "# Plotting time series data\n",
    "We'll smooth the data for temperature and number of people attending. A rolling window of 144 works out to be about a 1 day average, which is good for smaller views, but still a bit busy for a yearlong view"
   ],
   "id": "cd99e3c1057dca2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert the 'datetime' column to Pacific Time (PT)\n",
    "gym_data['datetime_pacific'] = gym_data['datetime'].dt.tz_convert('US/Pacific')\n",
    "\n",
    "# Apply a rolling mean for smoothing\n",
    "gym_data['number_people_smooth'] = gym_data['number_people'].rolling(window=144).mean()\n",
    "gym_data['temperature_smooth'] = gym_data['temperature'].rolling(window=144).mean()\n",
    "\n",
    "# Plotting the smoothed data as a function of the adjusted datetime column\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=gym_data, x='datetime_pacific', y='number_people_smooth', label='Smoothed Number of People')\n",
    "sns.lineplot(data=gym_data, x='datetime_pacific', y='temperature_smooth', label='Smoothed Temperature', color='orange')\n",
    "\n",
    "# Define a color map for seasons\n",
    "season_colors = {\n",
    "    'Spring': 'lightgreen',\n",
    "    'Summer': 'lightblue',\n",
    "    'Fall': 'orange',\n",
    "    'Winter': 'lightcoral'\n",
    "}\n",
    "\n",
    "# Helper function to determine the season\n",
    "def get_season(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Fall'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "# This calculates how many days fall in each season and returns the season with the highest count.\n",
    "def get_majority_season(start, end):\n",
    "    days = pd.date_range(start=start, end=end, freq='D')\n",
    "    season_counts = days.month.map(get_season).value_counts()\n",
    "    majority_season = season_counts.idxmax()  # Find the season with the highest count\n",
    "    return majority_season\n",
    "\n",
    "# Highlight the time periods that are semesters and label them with year and season\n",
    "for start, end in semester_intervals:\n",
    "    # Only draw if there is data in the given semester interval\n",
    "    interval_data = gym_data[(gym_data['datetime_pacific'] >= start) & (gym_data['datetime_pacific'] <= end)]\n",
    "    if not interval_data.empty:\n",
    "        season = get_majority_season(start, end)\n",
    "        color = season_colors[season]\n",
    "        label = f'{start.year} {season} Semester'\n",
    "\n",
    "        plt.axvspan(start.tz_convert('US/Pacific'), end.tz_convert('US/Pacific'), color=color, alpha=0.3, label=label)\n",
    "\n",
    "plt.title('Rolling window of 144 observations of Number of People and Temperature Over Time (Pacific Time)')\n",
    "plt.xlabel('Datetime (Pacific Time)')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "46abb32e439299f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Time series data\n",
    "It appears from the data the gym was gradually losing people attending, and then in 2017, there was something going on, possibly renovations that prevented data collection for a few months. We observe the expected dips around the holidays, near Thanksgiving and Christmas\n",
    "\n",
    "### Holidays\n",
    "Let's check if the `is_holiday` column is giving us the right information"
   ],
   "id": "c0fb50ef6e16afd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter rows where 'is_holiday' is 1\n",
    "holiday_dates = gym_data[gym_data['is_holiday'] == 1]['datetime'].dt.date\n",
    "\n",
    "# Group by year and extract unique dates for holidays\n",
    "holidays_per_year = holiday_dates.groupby(holiday_dates.map(lambda x: x.year)).unique()\n",
    "\n",
    "# Display the unique holiday dates per year\n",
    "for year, dates in holidays_per_year.items():\n",
    "    print(f\"Holidays in {year}: {list(dates)}\")"
   ],
   "id": "9c69fe70a9f2488d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Interestingly the `is_holiday` column seems to be nonfunctional, We would expect more holidays than just those. We'll need to repair this column too.\n",
    "\n",
    "We'll start with federal holidays. There's a good python module for this. We'll assume this is a US college for now, because we're observing a large dip at the end of november which corresponds to Thanksgiving, a US holiday. We can also assume from the timezone component being Pacific Time it's in the US as well.\n",
    "\n",
    "```\n",
    "pip install holidays\n",
    "```"
   ],
   "id": "78d8cebff69ec09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import holidays\n",
    "\n",
    "# Initialize the US holidays\n",
    "us_holidays = holidays.US()\n",
    "\n",
    "gym_data['is_holiday_fixed'] = gym_data['datetime'].dt.date.apply(lambda x: x in us_holidays)"
   ],
   "id": "4c81ac183e8d6d3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define state-specific holiday objects\n",
    "state_holidays = {\n",
    "    \"CA\": holidays.US(state='CA'),\n",
    "    \"WA\": holidays.US(state='WA'),\n",
    "    \"NV\": holidays.US(state='NV'),\n",
    "    \"OR\": holidays.US(state='OR'),\n",
    "    \"ID\": holidays.US(state='ID')\n",
    "}\n",
    "\n",
    "# Initialize federal holidays object\n",
    "federal_holidays = holidays.US()\n",
    "\n",
    "# Group by year and extract unique dates for holidays\n",
    "holidays_fixed_per_year = gym_data[gym_data['is_holiday_fixed']]['datetime'].dt.date.groupby(\n",
    "    gym_data['datetime'].dt.year\n",
    ").unique()\n",
    "\n",
    "# Create a list to store holiday data\n",
    "holiday_data = []\n",
    "\n",
    "# Generate holiday details\n",
    "for year, dates in holidays_fixed_per_year.items():\n",
    "    for date in dates:\n",
    "        state_details = {}\n",
    "        holiday_names = []\n",
    "\n",
    "        # Check which states have this holiday and get the holiday name\n",
    "        for state, state_holiday_obj in state_holidays.items():\n",
    "            holiday_name = state_holiday_obj.get(date)\n",
    "            state_details[state] = \"Yes\" if holiday_name else \"No\"\n",
    "            if holiday_name and holiday_name not in holiday_names:\n",
    "                holiday_names.append(holiday_name)\n",
    "\n",
    "        # Check federal holiday\n",
    "        federal_holiday_name = federal_holidays.get(date)\n",
    "        state_details[\"Federal\"] = \"Yes\" if federal_holiday_name else \"No\"\n",
    "        if federal_holiday_name and federal_holiday_name not in holiday_names:\n",
    "            holiday_names.append(federal_holiday_name)\n",
    "\n",
    "        # Combine holiday names into a single string\n",
    "        holiday_name_combined = \", \".join(holiday_names)\n",
    "\n",
    "        # Add the date, holiday name(s), and state details to the holiday data\n",
    "        holiday_data.append({\n",
    "            'Date': date,\n",
    "            'Holiday Name': holiday_name_combined,\n",
    "            **state_details,\n",
    "        })\n",
    "\n",
    "# Convert the holiday data into a DataFrame\n",
    "holiday_df = pd.DataFrame(holiday_data)\n",
    "\n",
    "# Display the dataframe nicely using the tabulate module\n",
    "from tabulate import tabulate\n",
    "print(tabulate(holiday_df, headers='keys', tablefmt='pretty'))"
   ],
   "id": "afcff6aa48187805"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Timezone and holidays\n",
    "Of the potential holidays and time zones, our 4 candidate states all observe the same as federal, so we can't narrow down which state the college belongs to.\n",
    "\n",
    "# Gym at busy periods\n",
    "Let's focus our view on 2016 and look for the busiest month, week, day so we can observe and check the data makes sense"
   ],
   "id": "a085513b6a16df70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter the dataset for the year 2016\n",
    "gym_data_2016 = gym_data[gym_data['datetime'].dt.year == 2016].copy()  # Avoid SettingWithCopyWarning by using .copy()\n",
    "\n",
    "# Apply timezone adjustment for Pacific Time before analysis\n",
    "gym_data_2016['datetime_pacific'] = gym_data_2016['datetime'].dt.tz_convert('US/Pacific')\n",
    "\n",
    "# Identify the busiest month in 2016 based on the average number of people\n",
    "busiest_month = gym_data_2016.groupby(gym_data_2016['datetime_pacific'].dt.month)['number_people'].mean().idxmax()\n",
    "gym_data_busiest_month = gym_data_2016[gym_data_2016['datetime_pacific'].dt.month == busiest_month].copy()  # Use .copy()\n",
    "\n",
    "# Filter for weeks in the busiest month that have 7 whole days\n",
    "gym_data_busiest_month['week'] = gym_data_busiest_month['datetime_pacific'].dt.isocalendar().week\n",
    "weeks_with_7_days = gym_data_busiest_month.groupby('week').filter(lambda x: x['datetime_pacific'].dt.date.nunique() == 7).copy()  # Use .copy()\n",
    "\n",
    "# Identify the busiest week in the busiest month with 7 days\n",
    "busiest_week = weeks_with_7_days.groupby('week')['number_people'].mean().idxmax()\n",
    "gym_data_busiest_week = weeks_with_7_days[weeks_with_7_days['week'] == busiest_week].copy()  # Use .copy()\n",
    "\n",
    "# Identify the busiest day in the busiest week\n",
    "busiest_day = gym_data_busiest_week.groupby(gym_data_busiest_week['datetime_pacific'].dt.date)['number_people'].mean().idxmax()\n",
    "gym_data_busiest_day = gym_data_busiest_week[gym_data_busiest_week['datetime_pacific'].dt.date == busiest_day].copy()  # Use .copy()\n",
    "\n",
    "# Display results\n",
    "print(f\"Busiest Month: {busiest_month}\")\n",
    "print(f\"Busiest Week: {busiest_week}\")\n",
    "print(f\"Busiest Day: {busiest_day}\")\n",
    "\n",
    "# Visualization for the entire year 2016 with seasons and semester overlays\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=gym_data_2016, x='datetime_pacific', y='number_people_smooth', label='2016 attendance', color='blue')\n",
    "\n",
    "# Highlight the time periods that are semesters and label them with year and season\n",
    "for start, end in semester_intervals:\n",
    "    if start.year == 2016:  # Only include semesters for 2016\n",
    "        season = get_majority_season(start, end)\n",
    "        color = season_colors[season]\n",
    "        label = f'{start.year} {season} Semester'\n",
    "        plt.axvspan(start, end, color=color, alpha=0.3, label=label)\n",
    "\n",
    "# Draw vertical bands exclusively for federal holidays\n",
    "shown_labels = []  # Track already shown labels for the legend\n",
    "for holiday_date in federal_holidays.keys():\n",
    "    if holiday_date.year == 2016:  # Only include holidays for 2016\n",
    "        holiday_datetime = pd.Timestamp(holiday_date).tz_localize('US/Pacific')\n",
    "        label = 'Federal Holiday' if 'Federal Holiday' not in shown_labels else \"\"\n",
    "        plt.axvline(holiday_datetime, color='red', linestyle='--', alpha=0.4, label=label)\n",
    "        if label:\n",
    "            shown_labels.append('Federal Holiday')\n",
    "\n",
    "plt.title('2016 year gym attendance')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Smoothed Number of People')\n",
    "plt.gca().xaxis.set_major_locator(plt.matplotlib.dates.MonthLocator())  # Show every month\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%B %Y'))  # Format as month name with year\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "7bdba21267e7ab08"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2016 insights\n",
    "The 2016 plot with holidays marked out makes sense. We can observe some trends.\n",
    "- That at start of the year and semester, attendance is high, which tails off as the semester completes. The fall semester duplicates this as well, but not as much\n",
    "- There is a baseline level of gym attendance, likely provided by staff and students not returning home for summer holidays\n",
    "- We also observe in the latter half of march a decline in attendance, likely coinciding with midterms and spring break."
   ],
   "id": "94867e9a90a40821"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualization for the Busiest Month\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=gym_data_busiest_month,\n",
    "             x=gym_data_busiest_month['datetime_pacific'] - pd.Timedelta(hours=7),  # Offset by timezone adjustment\n",
    "             y='number_people_smooth',\n",
    "             label='Busiest Month')\n",
    "\n",
    "# Highlight different weeks with transparent overlays and alternating colors\n",
    "unique_weeks = gym_data_busiest_month['datetime_pacific'].dt.isocalendar().week.unique()\n",
    "colors = sns.color_palette(\"hls\", len(unique_weeks))  # Generate unique colors\n",
    "\n",
    "for i, week in enumerate(unique_weeks):\n",
    "    week_data = gym_data_busiest_month[gym_data_busiest_month['datetime_pacific'].dt.isocalendar().week == week]\n",
    "    start_date = week_data['datetime_pacific'].min() - pd.Timedelta(hours=7)  # Offset start_date\n",
    "    end_date = week_data['datetime_pacific'].max() - pd.Timedelta(hours=7)  # Offset end_date\n",
    "    plt.axvspan(start_date, end_date, color=colors[i], alpha=0.3, label=f'Week {week}')\n",
    "\n",
    "# Adjust the x-axis to show correctly aligned ticks\n",
    "plt.gca().xaxis.set_major_locator(plt.matplotlib.dates.DayLocator(interval=7))  # Set major ticks for every 7 days\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%b %d'))  # Format major ticks as 'Month Day'\n",
    "\n",
    "# Add minor ticks for all 7 weekdays and ensure they display properly\n",
    "plt.gca().xaxis.set_minor_locator(plt.matplotlib.dates.DayLocator())  # Set minor ticks for every day\n",
    "plt.gca().xaxis.set_minor_formatter(plt.matplotlib.dates.DateFormatter('%a'))  # Format minor ticks as weekday names\n",
    "plt.tick_params(axis='x', which='minor', labelrotation=60)  # Rotate minor ticks 60 degrees\n",
    "plt.tick_params(axis='x', which='major', pad=25)  # Add padding to major ticks to position them further down\n",
    "plt.grid(visible=True, which='minor', color='gray', linestyle='--', linewidth=0.5, alpha=0.7)  # Enable minor gridlines\n",
    "\n",
    "# Change the plot title to include the busiest month\n",
    "busiest_month_name = pd.to_datetime(f'2016-{busiest_month}-01').strftime('%B')\n",
    "plt.title(f'Busiest Month in 2016: {busiest_month_name} with Weekly Overlays')\n",
    "plt.xlabel('Dates (Pacific Time)')\n",
    "plt.ylabel('Smoothed Number of People')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "cf9ee4c05c376487"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As expected, the weekly cycles of the gym indicates many people start on a monday and leave the weekends free. Interestingly, in the week of Feb 22, the Wednesday attendance was unusually low. It might correspond with a local event or examination period.",
   "id": "70ee43a9e4681f2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sanity check: How many people were in the gym on Saturday at 12 PM\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Use smoothed values for better visualization\n",
    "sns.lineplot(data=gym_data_busiest_week, x=gym_data_busiest_week['datetime_pacific'], y='number_people', label='Busiest Week', color='red')\n",
    "\n",
    "# Highlight data for individual days with proper timezone awareness\n",
    "unique_days = gym_data_busiest_week['datetime_pacific'].dt.normalize().unique()  # Unique normalized (midnight) date-like objects\n",
    "colors = sns.color_palette(\"hls\", len(unique_days))  #\n",
    "\n",
    "for i, day in enumerate(unique_days):\n",
    "    # Create start and end times for each day\n",
    "    day_data = gym_data_busiest_week[gym_data_busiest_week['datetime_pacific'].dt.normalize() == day]\n",
    "    start_time = day_data['datetime_pacific'].min()  # Proper timezone-aware start time\n",
    "    end_time = day_data['datetime_pacific'].max()  # Proper timezone-aware end time\n",
    "    weekday = day.strftime('%A')  # Get the weekday name\n",
    "    plt.axvspan(start_time, end_time, color=colors[i], alpha=0.3, label=f'{weekday}')\n",
    "\n",
    "plt.title('Busiest Week in 2016')\n",
    "plt.xlabel('Day of the Week (Busiest Month in 2016)')\n",
    "plt.ylabel('Number of People')  # Updated label\n",
    "\n",
    "# Add major and minor ticks for the x-axis\n",
    "plt.gca().xaxis.set_major_locator(\n",
    "    plt.matplotlib.dates.WeekdayLocator(byweekday=range(7), tz=start_time.tz)  # Timezone-aware locator for days\n",
    ")\n",
    "plt.gca().xaxis.set_minor_locator(\n",
    "    plt.matplotlib.dates.HourLocator(byhour=[6, 12, 18], tz=start_time.tz)  # Timezone-aware locator for hours\n",
    ")\n",
    "\n",
    "# Add gridlines for both major and minor ticks\n",
    "plt.grid(which='major', color='black', linestyle='-', linewidth=0.5)\n",
    "plt.grid(which='minor', color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Set custom labels for minor ticks\n",
    "plt.gca().xaxis.set_minor_formatter(plt.matplotlib.dates.DateFormatter('%#I%p', tz=start_time.tz))  # Timezone-aware formatter\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%A', tz=start_time.tz))  # Timezone-aware formatter\n",
    "plt.xticks(rotation=60)  # Rotate major x-axis labels for better readability\n",
    "plt.gca().tick_params(axis='x', which='minor', rotation=60)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "1c569cfcb575208e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As expected, there's minimal to no gym activity or data recorded between midnight and 6am. Gym activity picks up from 6am onwards each day with the evening crowd being larger than the morning crowd.",
   "id": "8f97e159d7b61860"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualization for the Busiest Day\n",
    "busiest_day_date = gym_data_busiest_day['datetime_pacific'].dt.date.unique()[0]  # Get the date of the busiest day\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=gym_data_busiest_day,\n",
    "             x=gym_data_busiest_day['datetime_pacific'],  # Use the timezone-aware datetime directly\n",
    "             y='number_people',\n",
    "             label='Busiest Day',\n",
    "             color='green')\n",
    "plt.title(f'Busiest Day in 2016: {busiest_day_date}')\n",
    "plt.xlabel('Day (Busiest Week in 2016)')\n",
    "plt.ylabel('Number of People')  # Reflect that raw data is being used\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.gcf().autofmt_xdate()  # Auto format date to show time\n",
    "\n",
    "# Set major ticks to start at midnight and every 3 hours thereafter\n",
    "plt.gca().xaxis.set_major_locator(plt.matplotlib.dates.HourLocator(byhour=range(0, 24, 3), tz=gym_data_busiest_day['datetime_pacific'].dt.tz))\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M', tz=gym_data_busiest_day['datetime_pacific'].dt.tz))  # Format as HH:MM with timezone awareness\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "d05e94db247b97f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "On a given day, activity starts around 6am and continues, with a bit of an increase after 5pm\n",
    "\n",
    "# EDA and data cleaning\n",
    "We expect that `temperature` and `date` to be the two primary features. We'll keep the `is_holiday`, `isstartof_semester`, `is_during_semester` features as well, since we can't recreate those just from the `date` alone. Let's take a look at correlations for all of them first"
   ],
   "id": "f3fbad92d5c2751d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display correlation matrix of all numeric columns with correlations greater than 0.1 in absolute value\n",
    "correlation_matrix = gym_data.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "# Create a mask to filter out correlations with absolute values <= 0.1\n",
    "mask = abs(correlation_matrix) <= 0.1\n",
    "\n",
    "# Apply the mask before plotting\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask, cbar_kws={\"label\": \"Correlation\"})\n",
    "plt.title('Correlation Matrix for Gym Data (|correlation| > 0.1)')\n",
    "plt.show()"
   ],
   "id": "f49f3057073e1008"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a large figure with subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 18))\n",
    "\n",
    "# Plotting the distribution of `number_people`\n",
    "sns.histplot(gym_data['number_people'], bins=30, kde=True, color='blue', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Distribution of Number of People')\n",
    "axes[0, 0].set_xlabel('Number of People')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Plotting the distribution of `temperature`\n",
    "sns.histplot(gym_data['temperature'], bins=30, kde=True, color='orange', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Distribution of Temperature')\n",
    "axes[0, 1].set_xlabel('Temperature (Fahrenheit)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Plotting the distribution of `month`\n",
    "sns.histplot(gym_data['month'], bins=12, discrete=True, kde=False, color='green', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Distribution of Month')\n",
    "axes[1, 0].set_xlabel('Month')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Plotting the distribution of `hour`\n",
    "sns.histplot(gym_data['hour'], bins=24, discrete=True, kde=False, color='purple', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Distribution of Hour')\n",
    "axes[1, 1].set_xlabel('Hour of the Day')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Plotting the distribution of `is_during_semester`\n",
    "sns.countplot(x='is_during_semester', data=gym_data, color='red', ax=axes[2, 0])\n",
    "axes[2, 0].set_title('Distribution of During Semester')\n",
    "axes[2, 0].set_xlabel('During Semester (1: Yes, 0: No)')\n",
    "axes[2, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Plotting the distribution of `day_of_week`\n",
    "sns.countplot(x='day_of_week', data=gym_data, color='brown', ax=axes[2, 1])\n",
    "axes[2, 1].set_title('Distribution of Day of Week')\n",
    "axes[2, 1].set_xlabel('Day of Week')\n",
    "axes[2, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a4fba5499363efdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Distributions of features\n",
    "- `number_of_people` shows a skewed distribution, where there's a number of distribution where the gym is empty (0 people)\n",
    "- `temperature` distribution shows some skew, likely caused by the uneven number of observations in summer and winter months\n",
    "- `month` distribution indicates substantially more observations from august to december\n",
    "- `hour` distribution shows consistent observations except between midnight to 4am, where they are reduced.\n",
    "- `is_during_semester` indicates that two thirds of the observations are in semester time.\n",
    "- `day_of_week` indicates fairly normal distribution of observations\n",
    "\n",
    "## Correlations\n",
    "The number of people is strongly correlated with the `timestamp`, then `temperature`, then `is_during_semester`. To no great surprise, `timestamp` and `hour` are perfectly correlated. Interestingly, there is a slight negative correlation between attendance on a weekend. Suggesting students might travel home or off campus on weekends.\n",
    "\n",
    "# Cleaning data\n",
    "We'll want to build a data set that contains only the features we care about or think will predict the number of people well"
   ],
   "id": "968ec918e96765e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "id": "73f26cad9a0d2594"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gym_data_clean = gym_data.copy()\n",
    "\n",
    "columns_to_drop = ['timezone', 'temperature_smooth', 'number_people_smooth', 'shift', 'datetime_pacific', 'is_holiday', 'timestamp', 'date']\n",
    "\n",
    "gym_data_clean.drop(columns=columns_to_drop, inplace=True)"
   ],
   "id": "20e20f00675aebc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "gym_data_clean['is_holiday_fixed'] = gym_data_clean['is_holiday_fixed'].apply(lambda x: 1 if x else 0)",
   "id": "3275ffe4904d5bb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since we observed dome decay over time for attendance from the start of the semester, let's add some features to the dataset to track that",
   "id": "a35918e7f8d24acc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to calculate days since the start of the most recent semester\n",
    "def calculate_days_since_start(datetime, intervals):\n",
    "    most_recent_days = None  # Store greatest days difference if no match\n",
    "    for start, _ in intervals:\n",
    "        difference = (datetime - start).days\n",
    "        if difference >= 0 and (most_recent_days is None or difference < most_recent_days):\n",
    "            most_recent_days = difference\n",
    "    return most_recent_days  # Return days since start of the most recent semester\n",
    "\n",
    "# Apply the function to create the new column\n",
    "gym_data_clean['days_since_start_of_semester'] = gym_data_clean['datetime'].apply(\n",
    "    lambda x: calculate_days_since_start(x, semester_intervals)\n",
    ")"
   ],
   "id": "7de8a15992398017"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to calculate days until the start of the next semester\n",
    "def calculate_days_until_next_semester(datetime, intervals):\n",
    "    for start, end in sorted(intervals):\n",
    "        if datetime < start:  # Check if the datetime is before the start of the semester\n",
    "            return (start - datetime).days\n",
    "    # If datetime is after all semesters, still return a value\n",
    "    return float('inf')  # specified numeric infinity or high-backdate timedelta.\n",
    "\n",
    "# Apply the function to create the new column\n",
    "gym_data_clean['days_until_next_semester'] = gym_data_clean['datetime'].apply(\n",
    "    lambda x: calculate_days_until_next_semester(x, semester_intervals)\n",
    ")"
   ],
   "id": "84f73d1d22531081"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display correlation matrix of all numeric columns with correlations greater than 0.1 in absolute value\n",
    "correlation_matrix_clean = gym_data_clean.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "# Create a mask to filter out correlations with absolute values <= 0.1\n",
    "mask = abs(correlation_matrix_clean) <= 0.1\n",
    "\n",
    "# Apply the mask before plotting\n",
    "sns.heatmap(correlation_matrix_clean, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask, cbar_kws={\"label\": \"Correlation\"})\n",
    "plt.title('Correlation Matrix for Gym Data Clean (|correlation| > 0.1)')\n",
    "plt.show()"
   ],
   "id": "3d9053d9a85f327a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gym_data_clean.drop(columns=['datetime'], inplace=True)\n",
    "\n",
    "# Define features and target variable\n",
    "X = gym_data_clean.drop(columns='number_people')\n",
    "y = gym_data_clean['number_people']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and testing datasets\n",
    "print(\"Training features shape:\", X_train.shape)\n",
    "print(\"Testing features shape:\", X_test.shape)\n",
    "print(\"Training target shape:\", y_train.shape)\n",
    "print(\"Testing target shape:\", y_test.shape)"
   ],
   "id": "f79302e4b5e6951e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create and fit the SGD Regressor to the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Using huber loss function to deal with outliers\n",
    "sgd_regressor = SGDRegressor(loss='huber', alpha=0.0001, max_iter=10000, tol=1e-3, random_state=42, learning_rate='optimal')\n",
    "sgd_regressor.fit(X_train_scaled, y_train)"
   ],
   "id": "fb661d15077e0cfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = sgd_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared:\", r2)"
   ],
   "id": "f37248870d9e3f75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display the labels and coefficients of the model\n",
    "print(\"\\nFeature Labels and Coefficients:\")\n",
    "for label, coef in zip(X.columns, sgd_regressor.coef_):\n",
    "    print(f\"{label}: {coef}\")"
   ],
   "id": "10549f2df4c65cd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model performance\n",
    "The mean absolute error indicates we get the number of people off by about 11, and R2 is only 51.07% correlated. This model is not performing well"
   ],
   "id": "fd5ca6f27637eb1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test.values, label='True Values')\n",
    "plt.plot(y_pred, label='Predicted Values', alpha=0.7)\n",
    "plt.title('Comparison of True and Predicted Values')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Number of People')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "3d3fdd5a593c3152"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The model is consistently predicting fewer people attending than actually did. We can try some hyperparameter optimisation, but stochastic gradient descent is probably the wrong tool for this problem.\n",
    "\n",
    "## Hyperparameter optimisation"
   ],
   "id": "f564f2fc3d08c73d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### Note this hyperparameter search will take a while ~2 mins on i9-14900K\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'loss': ['huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "    'max_iter': [5000, 10000],\n",
    "    'tol': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the SGDRegressor and parameter grid for R-squared\n",
    "grid_search_r2 = GridSearchCV(SGDRegressor(random_state=42), param_grid, cv=5, scoring='r2')\n",
    "grid_search_r2.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and the best R-squared score\n",
    "print(\"Best parameters found (R-squared): \", grid_search_r2.best_params_)\n",
    "print(\"Best R-squared score: \", grid_search_r2.best_score_)"
   ],
   "id": "c7d65f65ca775a4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Refit the model with the best parameters to extract feature weights\n",
    "best_r2_model = grid_search_r2.best_estimator_\n",
    "print(\"Feature weights with R-squared optimization:\")\n",
    "for feature_name, coef in zip(X_train.columns, best_r2_model.coef_):\n",
    "    print(f\"Feature {feature_name}: {coef}\")"
   ],
   "id": "75326289c2844cfe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Optimisation results\n",
    "Even after a fairly lengthy search of the hyperparameter optimisation space ,the delta for R2 initial guess and optimised ones is small only about 0.01\n",
    "\n",
    "# Model assumptions\n",
    "Stochastic Gradient Descent assumes a linear relationship between features and target variable, in this case number of people at the gym. We're also encoding variables in time that are cyclical as linear. Let's try some cyclical encoding of them to see how they go"
   ],
   "id": "3497686c7346c8c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gym_data_clean2 = gym_data_clean.copy()\n",
    "\n",
    "# Cyclical encoding for 'hour'\n",
    "gym_data_clean2['hour_sin'] = np.sin(2 * np.pi * gym_data_clean2['hour'] / 24)\n",
    "gym_data_clean2['hour_cos'] = np.cos(2 * np.pi * gym_data_clean2['hour'] / 24)\n",
    "\n",
    "# Cyclical encoding for 'month'\n",
    "gym_data_clean2['month_sin'] = np.sin(2 * np.pi * gym_data_clean2['month'] / 12)\n",
    "gym_data_clean2['month_cos'] = np.cos(2 * np.pi * gym_data_clean2['month'] / 12)\n",
    "\n",
    "# Cyclical encoding for 'day_of_week'\n",
    "gym_data_clean2['day_of_week_sin'] = np.sin(2 * np.pi * gym_data_clean2['day_of_week'] / 7)\n",
    "gym_data_clean2['day_of_week_cos'] = np.cos(2 * np.pi * gym_data_clean2['day_of_week'] / 7)"
   ],
   "id": "db8f585ec80b8876"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# gym_data_clean2.drop(columns=['date', 'datetime', 'timestamp'], inplace=True)\n",
    "\n",
    "# Define features and target variable\n",
    "X = gym_data_clean2.drop(columns='number_people')\n",
    "y = gym_data_clean2['number_people']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and testing datasets\n",
    "print(\"Training features shape:\", X_train.shape)\n",
    "print(\"Testing features shape:\", X_test.shape)\n",
    "print(\"Training target shape:\", y_train.shape)\n",
    "print(\"Testing target shape:\", y_test.shape)\n",
    "\n",
    "# Create and fit the SGD Regressor to the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Using huber loss function to deal with outliers\n",
    "sgd_regressor = SGDRegressor(loss='huber', alpha=0.0001, max_iter=10000, tol=1e-3, random_state=42, learning_rate='optimal')\n",
    "sgd_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = sgd_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Model performance"
   ],
   "id": "90cbbf01a1ac51b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "By moving to cyclical encoding we've improved the R2 and mean absolute error by orders of magnitude more than a hyperparameter search. It is likely this model can be improved by doing some additional feature engineering.",
   "id": "49809b3a6ef3e30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# List and show the coefficients and column names for the new model\n",
    "print(\"Coefficients for the updated model with cyclical encoding:\")\n",
    "for feature_name, coef in zip(X_train.columns, sgd_regressor.coef_):\n",
    "    print(f\"Feature {feature_name}: {coef}\")"
   ],
   "id": "649eea4300d4385c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create temporal aggregates\n",
    "# Calculate average number of people for each hour, and day of the week\n",
    "hourly_average = gym_data_clean2.groupby('hour')['number_people'].mean().reset_index(name='hourly_average')\n",
    "daily_average = gym_data_clean2.groupby('day_of_week')['number_people'].mean().reset_index(name='daily_average')\n",
    "\n",
    "# Merge these aggregates back to the original data\n",
    "gym_data_clean2_with_aggregates = pd.merge(gym_data_clean2, hourly_average, on='hour', how='left')\n",
    "gym_data_clean2_with_aggregates = pd.merge(gym_data_clean2_with_aggregates, daily_average, on='day_of_week', how='left')\n",
    "\n",
    "# Define features and target variable with the new aggregates\n",
    "X_with_aggregates = gym_data_clean2_with_aggregates.drop(columns='number_people')\n",
    "y_with_aggregates = gym_data_clean2_with_aggregates['number_people']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_agg, X_test_agg, y_train_agg, y_test_agg = train_test_split(X_with_aggregates, y_with_aggregates, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "X_train_agg_scaled = scaler.fit_transform(X_train_agg)\n",
    "X_test_agg_scaled = scaler.transform(X_test_agg)\n",
    "\n",
    "# Use the SGD Regressor again with modified inputs\n",
    "sgd_regressor_agg = SGDRegressor(loss='huber', alpha=0.0001, max_iter=10000, tol=1e-3, random_state=42, learning_rate='optimal')\n",
    "sgd_regressor_agg.fit(X_train_agg_scaled, y_train_agg)\n",
    "\n",
    "# Evaluate the new model\n",
    "y_pred_agg = sgd_regressor_agg.predict(X_test_agg_scaled)\n",
    "\n",
    "# Calculate evaluation metrics for the new model\n",
    "mse_agg = mean_squared_error(y_test_agg, y_pred_agg)\n",
    "mae_agg = mean_absolute_error(y_test_agg, y_pred_agg)\n",
    "r2_agg = r2_score(y_test_agg, y_pred_agg)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Mean Squared Error (with aggregates):\", mse_agg)\n",
    "print(\"Mean Absolute Error (with aggregates):\", mae_agg)\n",
    "print(\"R-squared (with aggregates):\", r2_agg)"
   ],
   "id": "2eb51a0a8b56986b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "By applying some temporal aggregates we've improved the model again. It makes sense since any given gym session might be longer than 1 hour. We don't have enough yearly data and the model is still crude.",
   "id": "cb8c5ec81b5cde28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "Stochastic Gradient Descent and a hyperparameter search yielded a model to predict gym attendance with an R2 of 0.508. Engineering some cyclical features from the time components and smoothing out the number of people data yielded and R2 of 0.613, a significant improvement\n",
    "\n",
    "Move improvements could likely be achieved by optimising the feature set of the training data. Given that smoothing out the temporal noise yielded significant improvements, the 10 minute sampling frequency of the data is complicating the fit. Additionally, many of the components going into the training data have some loose correlation in time. Holidays had minimal impact directly, but a field that counted days to next holiday would likely improve it.\n",
    "\n",
    "## Next steps\n",
    "### Varying temporal smoothing\n",
    "Rather than a hyperparameter search, doing a search over the different types of smoothing and seeing what the SGDRegressor comes out with.\n",
    "### Clustering\n",
    "There are clearly multiple populations of gym attendance, students vs non students, morning vs night, committed vs uncommitted. Developing a model that has different weightings to represent a linear combination of the different population types. At the very least, we could establish the population that is absent during summer holidays as a proportion of total baseline that's always present.\n"
   ],
   "id": "60afa9fe0ea1c3f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Personal note\n",
    "As this is a study project, I've exceeded my time budget for it. I may revisit it in future"
   ],
   "id": "d3bcffb1f2041f26"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
